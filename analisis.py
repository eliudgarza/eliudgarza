# -*- coding: utf-8 -*-
"""Analisis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/eliudgarza/eliudgarza/blob/main/Analisis.ipynb

Eliud Garza A00827575
"""

from google.colab import drive
drive.mount("/content/gdrive")

!pwd

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/gdrive/MyDrive/7mo Semestre/Modulo 2"

!ls

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

import missingno as msno 
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import zero_one_loss
from sklearn.preprocessing import LabelEncoder
from mlxtend.plotting import plot_decision_regions
from sklearn.preprocessing import LabelEncoder

from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
from sklearn import tree
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import validation_curve
from sklearn import datasets

def _draw_bootstrap_sample(rng, X, y):
    sample_indices = np.arange(X.shape[0])
    bootstrap_indices = rng.choice(
        sample_indices, size=sample_indices.shape[0], replace=True
    )
    return X[bootstrap_indices], y[bootstrap_indices]


def bias_variance_decomp(
    estimator,
    X_train,
    y_train,
    X_test,
    y_test,
    loss="0-1_loss",
    num_rounds=200,
    random_seed=None,
    **fit_params
):
    """
    estimator : object
        A classifier or regressor object or class implementing both a
        `fit` and `predict` method similar to the scikit-learn API.
    X_train : array-like, shape=(num_examples, num_features)
        A training dataset for drawing the bootstrap samples to carry
        out the bias-variance decomposition.
    y_train : array-like, shape=(num_examples)
        Targets (class labels, continuous values in case of regression)
        associated with the `X_train` examples.
    X_test : array-like, shape=(num_examples, num_features)
        The test dataset for computing the average loss, bias,
        and variance.
    y_test : array-like, shape=(num_examples)
        Targets (class labels, continuous values in case of regression)
        associated with the `X_test` examples.
    loss : str (default='0-1_loss')
        Loss function for performing the bias-variance decomposition.
        Currently allowed values are '0-1_loss' and 'mse'.
    num_rounds : int (default=200)
        Number of bootstrap rounds (sampling from the training set)
        for performing the bias-variance decomposition. Each bootstrap
        sample has the same size as the original training set.
    random_seed : int (default=None)
        Random seed for the bootstrap sampling used for the
        bias-variance decomposition.
    fit_params : additional parameters
        Additional parameters to be passed to the .fit() function of the
        estimator when it is fit to the bootstrap samples.
    Returns
    ----------
    avg_expected_loss, avg_bias, avg_var : returns the average expected
        average bias, and average bias (all floats), where the average
        is computed over the data points in the test set.
    Examples
    -----------
    For usage examples, please see
    http://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/
    """
    supported = ["0-1_loss", "mse"]
    if loss not in supported:
        raise NotImplementedError("loss must be one of the following: %s" % supported)

    for ary in (X_train, y_train, X_test, y_test):
        if hasattr(ary, "loc"):
            raise ValueError(
                "The bias_variance_decomp does not "
                "support pandas DataFrames yet. "
                "Please check the inputs to "
                "X_train, y_train, X_test, y_test. "
                "If e.g., X_train is a pandas "
                "DataFrame, try passing it as NumPy array via "
                "X_train=X_train.values."
            )

    rng = np.random.RandomState(random_seed)

    if loss == "0-1_loss":
        dtype = np.int64
    elif loss == "mse":
        dtype = np.float64

    all_pred = np.zeros((num_rounds, y_test.shape[0]), dtype=dtype)

    for i in range(num_rounds):
        X_boot, y_boot = _draw_bootstrap_sample(rng, X_train, y_train)

        # Keras support
        if estimator.__class__.__name__ in ["Sequential", "Functional"]:

            # reset model
            for ix, layer in enumerate(estimator.layers):
                if hasattr(estimator.layers[ix], "kernel_initializer") and hasattr(
                    estimator.layers[ix], "bias_initializer"
                ):
                    weight_initializer = estimator.layers[ix].kernel_initializer
                    bias_initializer = estimator.layers[ix].bias_initializer

                    old_weights, old_biases = estimator.layers[ix].get_weights()

                    estimator.layers[ix].set_weights(
                        [
                            weight_initializer(shape=old_weights.shape),
                            bias_initializer(shape=len(old_biases)),
                        ]
                    )

            estimator.fit(X_boot, y_boot, **fit_params)
            pred = estimator.predict(X_test).reshape(1, -1)
        else:
            pred = estimator.fit(X_boot, y_boot, **fit_params).predict(X_test)
        all_pred[i] = pred

    if loss == "0-1_loss":
        main_predictions = np.apply_along_axis(
            lambda x: np.argmax(np.bincount(x)), axis=0, arr=all_pred
        )

        avg_expected_loss = np.apply_along_axis(
            lambda x: (x != y_test).mean(), axis=1, arr=all_pred
        ).mean()

        avg_bias = np.sum(main_predictions != y_test) / y_test.size

        var = np.zeros(pred.shape)

        for pred in all_pred:
            var += (pred != main_predictions).astype(np.int)
        var /= num_rounds

        avg_var = var.sum() / y_test.shape[0]

    else:
        avg_expected_loss = np.apply_along_axis(
            lambda x: ((x - y_test) ** 2).mean(), axis=1, arr=all_pred
        ).mean()

        main_predictions = np.mean(all_pred, axis=0)

        avg_bias = np.sum((main_predictions - y_test) ** 2) / y_test.size
        avg_var = np.sum((main_predictions - all_pred) ** 2) / all_pred.size

    return avg_expected_loss, avg_bias, avg_var

dset = pd.read_csv("brain_stroke.csv")

dset.head(5000)

msno.bar(dset)

dset.info()

h_dset = dset.drop(["hypertension","heart_disease","gender","ever_married","work_type","Residence_type","stroke","smoking_status"],axis = 1)

h_dset.hist(bins=35, figsize=(20,13))
plt.show()

print(f"Skewness: {dset['age'].skew()}")
print(f"Kurtosis: {dset['age'].kurt()}")

print(f"Skewness: {dset['bmi'].skew()}")
print(f"Kurtosis: {dset['bmi'].kurt()}")

print(f"Skewness: {dset['avg_glucose_level'].skew()}")
print(f"Kurtosis: {dset['avg_glucose_level'].kurt()}")

h_dset.describe()

clean_cat = {"gender": {"Male":0, "Female": 1}, "ever_married": {"No":0,"Yes":1}, "work_type": {"Private":0, "Govt_job":1, "Self-employed":2, "children":3, "Neverworked":4}, "Residence_type": {"Rural":0, "Urban":1}, "smoking_status": {"never smoked":0, "Unknown":1,"formerly smoked":2, "smokes":3}}

dset = dset.replace(clean_cat)

dset.head()

dset["gender"].value_counts()

dset["gender"].value_counts(normalize = True)

dset["gender"].value_counts().plot(kind="bar")
plt.title("Gender")
plt.xlabel("Values")
plt.xticks(rotation=0)
plt.ylabel("Frequency")
plt.show()

dset["stroke"].value_counts()

dset["stroke"].value_counts(normalize = True)

dset["stroke"].value_counts().plot(kind="pie")
plt.title("Strokes vs No Strokes")
plt.xlabel("Values")
plt.xticks(rotation=0)
plt.ylabel("Frequency")
plt.show()

dset["ever_married"].value_counts()

dset["ever_married"].value_counts(normalize = True)

dset["ever_married"].value_counts().plot(kind="bar")
plt.title("Ever Married")
plt.xlabel("Values")
plt.xticks(rotation=0)
plt.ylabel("Frequency")
plt.show()

dset["smoking_status"].value_counts()

dset["smoking_status"].value_counts(normalize = True)

dset["smoking_status"].value_counts().plot(kind="pie")
plt.title("Smoking Status")
plt.xlabel("Values")
plt.xticks(rotation=0)
plt.ylabel("Frequency")
plt.show()

dset["heart_disease"].value_counts()

dset["heart_disease"].value_counts(normalize = True)

dset["heart_disease"].value_counts().plot(kind="bar")
plt.title("Heart Disease")
plt.xlabel("Values")
plt.xticks(rotation=0)
plt.ylabel("Frequency")
plt.show()

cor = dset.corr()
cor

# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(cor, dtype=bool))

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(200, 10, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(cor, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})

X = dset.drop(["stroke"], axis = 1)
Y = dset["stroke"]

#X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)

data = dset.values
X, y = data[:, :-1], data[:, -1]
le=LabelEncoder()
y=le.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.3,
                                                    random_state=123,
                                                    shuffle=True,
                                                    stratify=y)

clf_dt = DecisionTreeClassifier(random_state=123)
clf_dt.fit(X_train,y_train)
y_pred=clf_dt.predict(X_test)

avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(clf_dt, X_train, y_train, X_test, y_test, 
                                                            loss='0-1_loss',random_seed=123)

print("Accuracy:", accuracy_score(y_test,y_pred))
print("F1 Score:", f1_score(y_test,y_pred))
print("Recall:", recall_score(y_test,y_pred))
print("Precision:",precision_score(y_test,y_pred))
print("Confusion Matrix:\n",confusion_matrix(y_test,y_pred))
print('Average expected loss: %.3f' % avg_expected_loss)
print('Average bias: %.3f' % avg_bias)
print('Average variance: %.3f' % avg_var)
print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred))

clf_dt.fit(X_train, y_train)
train_predictions = clf_dt.predict(X_train)
test_predictions = clf_dt.predict(X_test)
train_acc = accuracy_score(y_train, train_predictions)
test_acc = accuracy_score(y_test, test_predictions)
print('train acc', train_acc)
print('test acc', test_acc)

### Después Pruning ###
clf_dt_prnd = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=123)
clf_dt_prnd.fit(X_train,y_train)
y_pred=clf_dt_prnd.predict(X_test)

avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(
        clf_dt_prnd, X_train, y_train, X_test, y_test, 
        loss='0-1_loss',
        random_seed=123)


print('Average expected loss--After pruning: %.3f' % avg_expected_loss)
print('Average bias--After pruning: %.3f' % avg_bias)
print('Average variance--After pruning: %.3f' % avg_var)
print('Sklearn 0-1 loss--After pruning: %.3f' % zero_one_loss(y_test,y_pred))

clf_dt_prnd.fit(X_train, y_train)
train_predictions = clf_dt_prnd.predict(X_train)
test_predictions = clf_dt_prnd.predict(X_test)
train_acc = accuracy_score(y_train, train_predictions)
test_acc = accuracy_score(y_test, test_predictions)
print('train acc', train_acc)
print('test acc', test_acc)

clf_dt_prnd = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=123)
clf_dt_prnd.fit(X_train,y_train)
y_pred=clf_dt_prnd.predict(X_test)

avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(
        clf_dt_prnd, X_train, y_train, X_test, y_test, 
        loss='0-1_loss',
        random_seed=123)


print('Average expected loss--After pruning: %.3f' % avg_expected_loss)
print('Average bias--After pruning: %.3f' % avg_bias)
print('Average variance--After pruning: %.3f' % avg_var)
print('Sklearn 0-1 loss--After pruning: %.3f' % zero_one_loss(y_test,y_pred))

clf_dt_prnd.fit(X_train, y_train)
train_predictions = clf_dt_prnd.predict(X_train)
test_predictions = clf_dt_prnd.predict(X_test)
train_acc = accuracy_score(y_train, train_predictions)
test_acc = accuracy_score(y_test, test_predictions)
print('train acc', train_acc)
print('test acc', test_acc)

clf_RF = RandomForestClassifier(max_depth=2, random_state=0)
clf_RF.fit(X_train,y_train)
y_pred=clf_RF.predict(X_test)

avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(
        clf_RF, X_train, y_train, X_test, y_test, 
        loss='0-1_loss',
        random_seed=123)



print('Average expected loss: %.3f' % avg_expected_loss)
print('Average bias: %.3f' % avg_bias)
print('Average variance: %.3f' % avg_var)
print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred))

clf_RF.fit(X_train, y_train)
train_predictions = clf_RF.predict(X_train)
test_predictions = clf_RF.predict(X_test)
train_acc = accuracy_score(y_train, train_predictions)
test_acc = accuracy_score(y_test, test_predictions)
print('train acc', train_acc)
print('test acc', test_acc)

train_accuracies = []
test_accuracies = []

for depth in range(1,25):
  tree_model = DecisionTreeClassifier(max_depth=depth)
  tree_model.fit(X_train, y_train)

  train_predictions = tree_model.predict(X_train)
  test_predictions = tree_model.predict(X_test)

  train_accuracy = metrics.accuracy_score(y_train, train_predictions)
  test_accuracy = metrics.accuracy_score(y_test,test_predictions)

  train_accuracies.append(train_accuracy)
  test_accuracies.append(test_accuracy)



plt.figure(figsize=(10,5))
sns.set_style("whitegrid")
plt.plot(train_accuracies, label = "train accuracy")
plt.plot(test_accuracies, label = "test accuracy")
plt.legend(loc = "upper left")
plt.xticks(range(0,26,5))
plt.xlabel("max_depth",size = 20)
plt.ylabel("accuracy",size = 20)
plt.show